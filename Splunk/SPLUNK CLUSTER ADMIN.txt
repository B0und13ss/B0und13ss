SPLUNK CLUSTER ADMIN 


STEP Lab PIN: CaptaincyTransfer 

EDU Cred: Penguin 

Lab Creds: student08  P: 






Splunk Deployment 

-Standalone (All functions on same host)= 


-SH & Indexer (same Host) and Forwarders= 



-SH ---> Indexers ---> Forwarders (Hosted on Deployment Server/Universal Forwarder)= Indexers have duplicate functions/indices to spread the load/data; but if one Indexer goes down, there is no redundancy and access to collected data, hence why we cluster!!!! 


Clustering

	-HA= Replicate buckets and indexers (can lose 1 Indexer OR 1 indexer less than copies of data); search head cluster to replicate Knowledge objects (Alias,Saved searches, dashboards,etc.)

	-DR= Multiple sites for cluster to move data to another site allowing (1) site failure;  



**UNIVERSAL FORWARDERS LOAD BALANCES DATA GOING TO INDEXERS, THE INDEXERS PERFORM THE REPLICATION!!! 


Manage Node 

	-Heartbeat lets this node that Indexers are alive and what buckets are used on the Indexers and how much space available, Manager balance out bucket and space usage 

Deployment Server 

	-Pushes configurations to Universal Forwarders (inputs.conf)
	-Outputs.conf tells forwarder what files to ingest and which Indexer to go to 
	-Basically an update Server like WSUS 

Deployer 
	-Pushes Configurations for the Search Heads 

Monitoring Console 

	-Basically a Manager Node for Search Heads
	


Distro Deployment Specs 

	- 
	- 
	- 


License Manager Configuration 

	Every 	Splunk instance by default
	same license pool and config
	splunkd.log & splunkd_access.log (Looking at logging) 
	Searches against license logs = index=_internal source=*license_usage.log    




Single Site Indexer Cluster 


	Manager= only 1, index replication coordinator, app bundles distributed, tells SH which peers to search 

	SH= required for index cluster, relies on manager for target of search peers  MINIMUM=2  

	Peer nodes= index data from forwarders, replicate data and buckets to peer nodes as directed by the Manager node  MINIMUM= 2

	Forwarders= Send data to peer nodes  

	Benefits= Data available, fast recovery, easier administration, no additional cost for replication 
	
	Cons= in creased storage and processing load, requires more Splunk instances, same OS and versions for Indexers 


	
	Requirements 
		>Each node runs on own host 
		>Manager must run same or newer version as peer nodes & SH (at least 3 minor versions newer) 
		>Synchronize system clocks on all machines
		> Enable clustering in UI/CLI or Server.conf 
		> Create & distribute config bundles 
	
	Best Practice 
		>Share same license pool
		>Dedicate roles/separate hardware to each instance 
		>Allocate storage for each peer node
		>Manager node on dedicated host 
		> Put License Manager and Monitoring console on same node OR Monitoring Console and Deployer on same node, could even do all 3 on same node 

	Key Specs 
	
		>Set under stanza in Server.conf of Manager Node
		>Set Replication Factor (total copies of rawdata , the fault tolerance level)
		>Set Search Factor (tsidx/Time-Series-Index File, defines where to find data in journal.**z file and how quickly, specifies searchable copies, cant be larger than replication factor) 
			Ex. 100gb raw data=compresses 15 gb 
			    100gb raw data=tsidx 35gb 

	Index Bucket Names 
		>ID primary copies of buckets to partipcate in search 
		>Non-searchable will lack ".tsidx"
		>db= original 
		>rb= replicated 
		>Timestamps indicate age of bucket in epoch seconds 
		>GUID 
		>Bucket Location= Homepath, coldpath,thawedpath of index 
			Ex. $SPLUNK_HOME/var/lib/splunk/<indexname>/* 


	Index Replication Health 
		> Complete= X Copies of each bucket, X-1 amount of which are searchable 
		>Valid= at least 1 searchable copy 
		>Bucket Fixup=





Multi-site Clustering **Supporting 1-63 sites**

	Requirements
		> 1 manager node, can tell a node is down by 2 missed heartbeats (Appx. 10 seconds)  
		> 2 peer modes/indexers
		> 1+ SHs **Can join cluster at any time, participate in multiple clusters, combine searches of peers clustered & non-clustered
		> Same OS, versions, search/Replication factors for Indexers

	
		

		
	
 Distributing  Config Bundles (Apps)

	-Manager Node distributes the bundles across indexer peers 
	-Staged in "SPLUNK_HOME/etc/manager-app/<APP NAME>", only subdirectory contents pushed to peers under SPLUNK_HOME/etc/peer-apps/<APP NAME>/local/indexes.conf", place standalone files in "_cluster/local" subdirectory (Config files to be distributed to peers) 
	-WebUI= Settings --> Indexer clustering --> Edit --> Configuration bundle actions OR CLI= "splunk apply cluster-bundle"; Can validate in same section of WebUI looking at "last_ validation_succeeded" and see if a restart is required "last_check_restart" OR CLI= "splunk validate cluster-bundle --check-restart" then "splunk show cluster-bundle-status" 
	-Settings --> Indexer clustering --> Edit --> Configuration --> Rollback in WebUI to go back to previous bundle, then fix issues in the manager-apps OR "splunk rollback cluster-bundle" 

	-Precedence order for indexer cluster peers:
		1. peer-apps local directories SPLUNK_HOME/etc/peer-apps/*/local/*.conf
		2. System local directory: SPLUNK_HOME/etc/system/local/*.conf
		3. App local directories: SPLUNK_HOME/etc/apps/*/local/*.conf
		4. peer-apps default directories: SPLUNK_HOME/etc/peer-apps/default/*.conf
		5. App default directories: SPLUNK_HOME/etc/apps/*/default/*.conf
		6. System default directory: SPLUNK_HOME/etc/system/default/*.conf 




Index Cluster configuration 

	-Replication configured in "indexes.conf"(auto= replicate, 0= DO NOT replicate)
	-$SPLUNK_HOME/etc/peer-apps/_cluster/default/indexes.conf, deploy indexes.conf from the manager node
	



SRR 
	-Restarts peer nodes while maintaining the search availability
	-Best chance to maintain a “valid” state throughout the restart
	-Settings --> Indexer clustering --> Edit --> Rolling Restart OR "splunk rolling-restart cluster-peers -searchable <true/false>" 
		> "-force" = allow SRR even if not "Complete" 
		> "restart_inactivity_timeout <sec>" = timeout for Manager to move on to next peer 
		>"decommission_force_timeout <sec>" = timeout for a peer to finish and do an early restart **Default is 180 sec/3 min**



Temp. Offline Peer

	-"splunk offline" on the peer in CLI
	-ENSURE CLUSTER IS IN "COMPLETE" STATE!
	-Manager Node will start bucket fixup in 60 seconds if the wait is not extended, "splunk edit cluster-config \ -restart_timeout <wait_seconds>" on manager node to adjust wait time




Decommission Peer 

	-"splunk offline --enforce-counts" on the peer in CLI
	-Must perform Bucket Fixup and validate Search and Rep Factor are "Complete" BEFORE decommissioning, then manually remove from cluster "splunk remove cluster-peers -peers <guid>", Restart Manager Node

	


Peer Detention 
	
	-Automatic= cross "minFreeSpace" allocation in server.conf (Default is 5 GB), stop internal/external indexing, replication stopped
	-Manual=Stop replication from peer nodes, disables external data ports and stops external indexing (optional) BUT continues internal indexing, continues searches
		>BEFORE decommissioning a peer to make it available for searching
		>Preemptively avoid automatic detention
		> Diagnose potentially corrupted peer through blocking of indexing & replication
		>Force new data to other peers
 		>Slow disk writes by only allowing indexing
  


Site Manipulation 

	-Edit "server.conf" on the Manager Node 

	-Replace Old Peers with New 
		>Adjust "available_sites" one OLD SITE to one NEW SITE,"site_replication_factor" to <ORIGIN:1, OLD SITE:1, NEW SITE:1, total:2>, "site_search_factor" to <ORIGIN:1, OLD SITE:1, NEW SITE:1, total:2>

	-Replace All Sites 
		>Adjust "available_sites" to new sites and "site_mappings" to <OLD SITE:NEW SITE> 

	-Consolidate Sites 
		>Adjust "available_sites" to new site and "site_mappings" to <default_mapping:NEW SITE> 

	-Decommissioned Site
		>1 searchable copy of each bucket needed at other sites. ideally "Complete" 
		>Manager node can't be part of this site 
		> Set "constrain_singlesite_buckets=true" 
		>Reassign site search heads to a remaining site
		> Update available_sites,site_replication_factor,site_search_factor,sites_mappings in "server.conf" 
		>Restart Manager Node
		>Run "splunk disable maintenance-mode", then decommission each peer with "splunk offline ––enforce-counts"




 Bucket Fixup
	
	-Manager node returning cluster to valid and complete state (if possible) by reassigning primaries and directing surviving indexers to
copy both index and rawdata to meet search and rep factors
	-Settings --> Indexer Clustering --> Indexes (Tab) --> Bucket Status in WebUI



Primary Rebalancing
	-Balances search load across all peer nodes
	-Manager node identifies duplicate searchable buckets, tries to reassign same # of primaries on each peer, DOES NOT move searchable copies to other peer nodes
	-Occurs independently for each site in a multisite cluster, DOES NOT shift primaries between sites
	-Triggers automatically when peer nodes join or rejoin the cluster and/or Manager node rejoins the cluster
	-Rolling restart completes, manually trigger rebalancing with REST endpoint on manager node = "services/cluster/manager/control/control/rebalance_primaries" 



Data Rebalancing

	-Redistributes the number of bucket copies per index
		>Balances storage distribution across peer nodes
		> ONLY warm & cold buckets, NOT hot buckets
		>Rebalances all non-searchable, searchable and primary buckets
		>Rebalances within sites as well as across sites in multisite cluster

	-Uneven bucket distribution may cause higher load on peer nodes & may occur:
		>After adding new peer nodes
		>When forwarding data is skewed
		>After frequent bucket fixups due to outages 

	-Useful in maintaining a balanced # of buckets per Index between nodes, BUT increases workload and search performance  




Excess Bucket Cleanup 
	
	- May result from peers leaving & returning to cluster, doesn't effect cluster operation but they consume storage space 
	-"splunk list excess-buckets [index]" then "splunk remove excess-buckets [index]"  


Cluster Dashboards 

	-Cluster Status provides the same information as the manager's indexer clustering page
 	-Cluster Service Activity = most of the panels should be blank (healthy)!!! 
	-Pay attention to an increasing trend on pending tasks and jobs 






Indexer Discovery 

	-Dynamic & automatic connection of forwarders to peer nodes in Indexer Cluster, minimizing forwarder restarts and allowing site-awareness to be configured 			
	-Edit "server.conf", pass4SymmKey= <DISCOVERY SECRET> 
	-Edit "outputs.conf", define target group name "tcpout: <TARGET GROUP>" --> "indexerDiscovery= <NAME>"
	-"polling_rate" = factor used to calculate polling interval ***poll_interval (seconds) = #_of_forwarders / polling_rate + 30*** 
	-Indexer Acknowledgement 
		>"useACK = true" = forwarder waits for indexer to acknowledge receipt data and writing before sending more data, if NO ACK, then data sent to another peer 		***OUTPUTS.CONF***  
		>"useACK = false" = forwarder keep sending data without receipt ***OUTPUTS.CONF***
		>"ack_factor" = sets # of copies of ingest data that must make it across indexer cluster before ACK is returned from source peer to Forwarder, only valid if "useAck = true", all peer nodes must use the same value ***SERVER.CONF***

	-Multisite Clustering Site Failover 
		> Multisite forwarders and Manager Node MUST use Indexer Discovery to enable failover
		>Site-aware forwarders WILL NOT failover to another site by default
		>Forwarding to original site resumes when node is back online 
		> Edit "server.conf" on Manager Node, run "splunk edit cluster-config -forwarder_site_failover <site-id>:<failover-id>" 

	***Metrics of Universal Forwarder for Site Failover = "index=_internal host=uf Metrics group=tcpout_connections | timechart span=1h sum(kb) by destPort"***
		 





SEARCH HEAD CLUSTER 

	-Raft distributed consensus= 1 member serves as Captain (elected dynamically, permanent unless Search Head goes down or Static during DR), coordinating cluster activity; must consist of AT LEAST (3) members
	-Deployer (NOT DEPLOYMENT SERVER) provides apps to SHs in cluster
	-Schedules jobs, Coordinate/supress alert actions, Push knowledge bundle/app (Knowledge Objects like macros,dashboards,etc.), Search artifact replication, Config Update replication  
	-Place majority of members in primary operating location if cluster is deployed across multiple physical sites 
	
	-Configuration 
		>Install Splunk Enterprise instances= same baseline default config, same Splunk Version, same hardware configuration 

		***"SPLUNK_HOME/etc/system/local/server.conf"***

		>Initialize members
			* "splunk init shcluster-config -mgmt_uri https://<SH DNS>:<PORT> -replication_port <PORT> –secret shcluster" 	
		>Bootstrap Captain  
			*"splunk bootstrap shcluster-captain –servers_list https://SH1:PORT,https://SH2:PORT,https://SH3:PORT"  ***ASSIGN CAPTAINCY***
		>Check SH Cluster status 
			*"splunk show shcluster-status"
			*"splunk list shcluster-members" 
		>Add Indexers/Peers from SHC member= "splunk add search-server https://<peer>:<PORT> -remoteUsername <user> -remotePassword <pw>"

	
	-Restarting SHC
  
		> Members restart in a phased approach to continue cluster operation
		>Captain is the FINAL member to restart and automatically invokes Captaincy transfer, thus preventing captaincy from changing during the restart 
 		>Deployer automatically initiates a rolling restart, when necessary 
		>"splunk rolling-restart shcluster-members" --> "splunk rolling-restart shcluster-members -searchable true -decommission_wait_time 180 -force false" -->  "splunk rolling-restart shcluster-members -status 1"
		

	
	-Summary Indexing 
		> Scheduled reports for indexing summary results 
		>Only summarizes data on that SH; Tell Search Head not to index and forward  summary reports and events to Indexers, where they can be replicated ***Outputs.conf***  



	- SHC Detention 
		>  Disables search but allows continued participation for maintenance and configuration
		> Minimal impact on end-user searches= completes in-progress searches, scheduled searches push to remaining members, participates in election and bundle replication
		>DOES NOT participate in Search artifact replication
		> "splunk edit shcluster-config -manual_detention [on|off]" --> "splunk show shcluster-status" --> "splunk list shcluster-member-info"




SHC Deployer

	-Distributes apps/bundles, configurations and non-replicable files 
		>Stage apps and configs in SPLUNK_HOME/etc/shcluster/apps, SPLUNK_HOME/etc/shcluster/users for private Knowledge Objects 
		>Configure push modes of apps under [shclustering] in "app.conf", "deployer_push_mode = merge_to_default (XXXXXX) | full (XXXXXX) | default_only (default directory to SHC default directory) | local_only (Transfer Search and Reporting App from local directory to SHC local directory) " 
		>Push bundles to SH Members, "splunk apply shcluster-bundle –target <member:port>" 
		>LOCAL directory takes Push mode preference unless otherwise specified 

	-Must run on a Non-SH Member 
	-Secret Key must match pass4SymmKey of SHC 
	-Direct edit of "server.conf" REQUIRED 
	-No Heartbeat for deployment, is a MANUAL Push 


Preserving Lookup Files 

	-CSV Files
	-Preserve/Overwrite lookup tables when upgrading apps 
	-"deployer_lookups_push_mode" in "apps.conf" under [shclustering]
		>always_preserve= preserves no matter "-preserve-lokups" flag actions 
		>always_overwrite=overwrites no matter "-preserve-lokups" flag actions
		>overwrite_on_change= overwrites if app contents chag, no matter "-preserve-lokups" flag actions
		>preserve_lookups=preserve only if " splunk apply shcluster-bundle -preserve-lookups true"
	
			
SH Cluster Restart 
	
	-XXXXXX
	-XXXXXX 




Bundle Deployment Status 
	
	-XXXXXX
	-XXXXXX 

Multithreaded Deployment 

	-Pushes apps to 1 member at a time by default
	-XXXXXX  






Deployment Server 

	-Pushes updates to forwarders, non-clustered indexers, non-clustered SHs 
	-CANNOT distribute apps or configs to SH Cluster Members
	-Depends on a phone-home interval 
	-Can distribute apps to "manager-apps" on Manager Node and/or "shcluster" on Deployer 
	-Configured in "deploymentClient.conf" 

SHC Member Management 

	-Set "preferred_captain=<true|false>" in "server.conf" under [shclustering] to control Captaincy
	- Transfer Captaincy "splunk transfer shcluster-captain -mgmt_uri <new_captain>"
	-Add SH Members 
		>Fresh Splunk instance 
		>Use same version, hardware configs as existing members 
		>Configure [shclustering] and [replication_port://<PORT>] in "server.conf" 
		>Restart

	 -Can add new members, Decommissioned member, Offline Members (can't reach merge point consensus in ops.json, might not be able to join cluster) 
		>Install Splunk Enterprise 
		>Initialize instance
		> Add to SHC 
			*"splunk add shcluster-member -new_member_uri https://<new_member>:8089" from EXISTING 
			*"splunk add shcluster-member -current_member_uri https://<any_existing_member>:8089" from NEW MEMBER 
		>Offline Member = "splunk resync shcluster-replicated-config" 
		>Decommisiioned Member 	
			*Run from any SHC member= "splunk remove shcluster-member -mgmt_uri <downedSH:m_port>" 
			*Temporary decommissioned member= "splunk remove shcluster-member" --> "splunk stop" 
			*Decommissioned= "splunk remove shcluster-member" --> "splunk disable shcluster-config" 

Rolling Upgrade of SHC

	-XXXXXX
	-XXXXXX 


SHC Health Report 
	
	-Status= Yellow (missed heartbeat), Red (multiple missed heartbeats)
	-Rep Factor=  Yellow (enough SHC Members for Artifact Replication) 
	-XXXX 
	-XXXX 

SHC Monitoring Console 

	-Status & Config= IF NOT GREEN, YOU'RE IN TROUBLE! Last Heartbeats should be similar, heartbeat gap and frequent captain election indicates problem
	-XXXX
	-XXXX 
	-XXXX  



KV Store

	-Stores data in COLLECTIONS, {KEY:VALUE} pairs 
	-Created in "collections.conf" in apps 
	-Perform CRUD Operations= XXXXX 
	-Utilizing in SHCs 
		>KV store forms own RAFT cluster, using up to (50) SHC Members
		>Handles tags.conf, savedsearches.conf,events.conf updates 
		>KV Store port must be available to all SHC Members, 8191 by Default 
		>SHC Captain and KV Store Captain/Primary sync member lists everytime there is a status/database change
			*Write= Primary receives all write ops and records  in journal, secondary members copy and apply ops asynchronously; Consensus is met when majority of nodes update KV Store data and log to journals (providing recovery info)
			*Read= performed on any member, routes to nearest member if KV Store data is not yet updated, but prefers local instance
		> Uses "mgmt_uri" values in  [shclustering] for KV Store connection and replication by Default  

	-Colection Replication 
		>NOT bundle-replicated to Indexers 
		>Lookups run locally on SH 
		> Edit "collections.conf" on each Member, USE DEPLOYER to deploy these configs to SHs 
		> Set Replication to "true"
 
			[mykv]
			enforceTypes = true
			field.x = number
			field.y = string
			accelerated_fields.xl2 = {"x": 1, "y":

		>Restart Splunk  







SmartStore 

	-Separate compute resources from storage for Indexers utilizing Cloud services 
	-Lower cost of data retention 
	-Greater Reliability, Availability and Scalability 
	-Compatible with AWS S3, Google Cloud GCS, Azure blob remote stores 
	-Cache Management= uploads buckets to remote storage when going hot --> warm, downloads fetch buckets if evicted (based on age, search freq., etc.) and needed for search 
	-Rollover to Remote store at Local Hot=750mb/10gb,90 Days OR Splunk restart  OR Local Warm=24 Hours (Warm bucket is read-only) 
	-Index Retention managed cluster-wide *** "– maxGlobalDataSizeMB" ***

	-Index Time Workflow 
		>Data write to Hot bucket, replicated 
		>Bucket rolls warm, uploaded to remote storage, copies contain directory structure and bloomfilters/metadata but no data
		>Warm bucket uploads to remote store, local bucket can be evicted 
		>Buckets freeze from warm state  

		***Manager node assigns a freeze job to a peer node that has a local bucket copy, identifies freeze candidates every 15 minutes by default***

	-Search Time Workflow 
		>SH delegates job to peers
		>Peers start search & read bucket list
		>Search process searches only "opened" buckets= not all searches require the entire content, fetches buckets "As needed", buckets open when available locally
		>Search finishes buckets are "closed" and can be evicted



	-Bootstrap
		>Fetches buckets present on remote storage to a New node, confirm all buckets fetched before bringing down old node 
		>Use same remote store on New nodes 
		>Manager Node coordinates discovery process on each Index 
		>Peers get buckets from remote store, recreates bucket metadata locally and sets it as Primary on one node, replicates primary's metadata to other peer nodes	



	-Search Considerations 
		>SmartStore is best-suited for Splunk searches that typically occur over near-term & recent data, Hot buckets which are always (and only) local/never evicted
		>Searches always occur in local storage 
		>Primary hot buckets participate in Searches 
		>Primary Indexer fetches bucket if needed for a search







	











